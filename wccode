move jar file and sample.txt in home

start-all.sh
 jps
hadoop fs -mkdir -p inp
hadoop fs -put sample.txt inp
hadoop jar WordCount.jar wordcount.WCDriver inp outp
hadoop fs -ls outp
hadoop fs -cat outp/part-00000



temp
start-all.sh
 jps
hadoop fs -mkdir -p input
hadoop jar CGPA.jar cgpa.CGPADriver input output
remove .CGPA in the driver class

//code
package cgpa;

import java.io.IOException;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class CGPAMapper extends Mapper<Object, Text, Text, DoubleWritable> {

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        // Split line into tokens (even if just 1 value)
        String[] tokens = value.toString().split("[,\\s]+"); // Split by comma or whitespace
        for (String token : tokens) {
            try {
                double cgpa = Double.parseDouble(token.trim());
                context.write(new Text("CGPA_MEAN"), new DoubleWritable(cgpa));
                context.write(new Text("CGPA_MAX"), new DoubleWritable(cgpa));
            } catch (NumberFormatException e) {
                // Skip invalid numbers
            }
        }
    }
}




package cgpa;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class CGPADriver {
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: CGPA <input path> <output path>");
            System.exit(-1);
        }

        Job job = Job.getInstance(new Configuration());
        job.setJarByClass(CGPADriver.class);
        job.setJobName("CGPA Analysis");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(CGPA.CGPAMapper.class);
        job.setReducerClass(CGPA.CGPAReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}


package cgpa;

import java.io.IOException;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class CGPAReducer extends Reducer<Text, DoubleWritable, Text, DoubleWritable> {
    public void reduce(Text key, Iterable<DoubleWritable> values, Context context)
            throws IOException, InterruptedException {
        if (key.toString().equals("CGPA_MEAN")) {
            double sum = 0;
            int count = 0;
            for (DoubleWritable val : values) {
                sum += val.get();
                count++;
            }
            context.write(new Text("Overall Mean CGPA"), new DoubleWritable(sum / count));
        } else if (key.toString().equals("CGPA_MAX")) {
            double max = Double.MIN_VALUE;
            for (DoubleWritable val : values) {
                max = Math.max(max, val.get());
            }
            context.write(new Text("Overall Max CGPA"), new DoubleWritable(max));
        }
    }
}


TopN
package-topn

package topn;

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class TopNMapper extends Mapper<Object, Text, Text, IntWritable> {
    private static final IntWritable one = new IntWritable(1);
    private Text word = new Text();

    private String tokens = "[_|$#<>\\^=\\[\\]*/\\\\,;,.\\-:()?!\"']";

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        String cleanLine = value.toString().toLowerCase().replaceAll(this.tokens, " ");
        StringTokenizer itr = new StringTokenizer(cleanLine);
        while (itr.hasMoreTokens()) {
            this.word.set(itr.nextToken().trim());
            context.write(this.word, one);
        }
    }
}
package samples.topn;

import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class TopNCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values)
            sum += val.get();
        context.write(key, new IntWritable(sum));
    }
}


package topn;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
import utils.MiscUtils;

public class TopNReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
    private Map<Text, IntWritable> countMap = new HashMap<>();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values)
            sum += val.get();
        this.countMap.put(new Text(key), new IntWritable(sum));
    }
For rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)


    @Override
    protected void cleanup(Context context) throws IOException, InterruptedException {
        Map<Text, IntWritable> sortedMap = MiscUtils.sortByValues(this.countMap);
        int counter = 0;
        for (Text key : sortedMap.keySet()) {
            if (counter++ == 20) // change 20 to N for configurable Top-N
                break;
            context.write(key, sortedMap.get(key));
        }For rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)


    }
}

create new package Utils, and click on it for new class MiscUtils and set package name as Utils
package utils;

import java.util.*;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

public class MiscUtils {

    public static Map<Text, IntWritable> sortByValues(Map<Text, IntWritable> map) {
        // Create a list from the entries of the map
        List<Map.Entry<Text, IntWritable>> entryList = new ArrayList<>(map.entrySet());

        // Sort the list in descending order of values
        Collections.sort(entryList, new Comparator<Map.Entry<Text, IntWritable>>() {
            public int compare(Map.Entry<Text, IntWritable> e1, Map.Entry<Text, IntWritable> e2) {
                return e2.getValue().compareTo(e1.getValueFor rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)

()); // Descending
            }
        });

        // Put the sorted entries into a LinkedHashMap to preserve order
        Map<Text, IntWritable> sortedMap = new LinkedHashMap<>();
        for (Map.Entry<Text, IntWritable> entry : entryList) {
            sortedMap.put(entry.getKey(), entry.getValue());
        }

        return sortedMap;
    }
}

package topn;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer; // Added for completeness
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class TopN {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();

        if (otherArgs.length != 2) {
            System.err.println("Usage: TopN <in> <out>");
            System.exit(2);
        }

        Job job = Job.getInstance(conf);
        job.setJobName("Top N");
        job.setJarByClass(TopN.class);
        job.setMapperClass(TopNMapper.class);
        job.setReducerClass(TopNReducer.class); // You must define TopNReducer separately
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }

    public static class TopNMapper extends Mapper<Object, Text, Text, IntWritable> {
        private static final IntWritable one = new IntWritable(1);
        private Text word = new Text();

        // Regular expression to clean tokens
        private String tokens = "[_|$#<>\\^=\\[\\]*/\\\\,;,.\\-:()?!\"']";

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String cleanLine = value.toString().toLowerCase().replaceAll(this.tokens, " ");
            StringTokenizer itr = new StringTokenizer(cleanLine);
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken().trim());
                context.write(word, one);
            }
        }
    }

    // You still need to implement the TopNReducer class
}

hadoop fs -mkdir -p in
hadoop@bmscecse-HP-Pro-Tower-400-G9-PCI-Desktop-PC:~$ hadoop fs -put sample.txt in
hadoop@bmscecse-HP-Pro-Tower-400-G9-PCI-Desktop-PC:~$ hadoop jar TopN.jar topn.TopN in ou





scala
:q
nano filename.scalaFor rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)


copy paste code 
object ExampleForLoop1 {
  def main(args: Array[String]) {
    var counter: Int = 0;For rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)


    for (counter <- 1 to 100)
      print(counter + " ");
  }
}
change object name to filename
ctrl +o,enter,cntr x
scalac filename.scala
scala filename



https://www.nologin.in/spark1

For rdd and flatmap

Exit Spark Shell if it’s open:
:q

nano sparkdata.txt
//write some lines in the file and then Save and exit the file: Ctrl + O, Enter, Ctrl + X

Launch Spark Shell :
spark-shell


//  Reads the file line-by-line into an RDD.
val data = sc.textFile("sparkdata.txt")

// Split each line into words
val splitdata = data.flatMap(line => line.split(" "))

// Convert each word into a (word, 1) pair
val mapdata = splitdata.map(word => (word, 1))

// Reduce by key to count occurrences of each word
val reducedata = mapdata.reduceByKey(_ + _)

// Filter to get words with count > 4
val filtered = reducedata.filter { case (word, count) => count > 4 }

// Collect and print the result
filtered.collect.foreach(println)







cassandra

onnected to Test Cluster at 127.0.0.1:9042
[cqlsh 6.1.0 | Cassandra 4.1.8 | CQL spec 3.4.6 | Native protocol v5]
Use HELP for help.
cqlsh> create keyspace students with replication={'class':'SimpleStrategy','replication_factor':1};
cqlsh> use students;
cqlsh:students> create table student(rollno int primary key,name text,date timestamp,last double);
cqlsh:students> begin batch insert into student(rollno,name,date, last) values(1,'Ashu','2012-03-12',89.9); apply batch;
cqlsh:students> select * from student;

 rollno | date                            | last | name
--------+---------------------------------+------+------
      1 | 2012-03-11 18:30:00.000000+0000 | 89.9 | Ashu

(1 rows)
cqlsh:students> update student set name='dinesh' where rollno=1;
cqlsh:students> select * from student;

 rollno | date                            | last | name
--------+---------------------------------+------+--------
      1 | 2012-03-11 18:30:00.000000+0000 | 89.9 | dinesh

(1 rows)
cqlsh:students> update student set rollno=2 where rollno=1;
InvalidRequest: Error from server: code=2200 [Invalid query] message="PRIMARY KEY part rollno found in SET part"
cqlsh:students> alter table student add lang list<text>;
cqlsh:students> select * from student;

 rollno | date                            | lang | last | name
--------+---------------------------------+------+------+--------
      1 | 2012-03-11 18:30:00.000000+0000 | null | 89.9 | dinesh

(1 rows)
cqlsh:students> update student set lang=lang+['tamil','eng']where rollno=1;
cqlsh:students> select * from student;

 rollno | date                            | lang             | last | name
--------+---------------------------------+------------------+------+--------
      1 | 2012-03-11 18:30:00.000000+0000 | ['tamil', 'eng'] | 89.9 | dinesh

(1 rows)
cqlsh:students> begin batch insert into student(rollno,name,date, last) values(1,'Ashu','2012-03-12',89.9)using ttl 30; apply batch;
cqlsh:students> select * from student;

 rollno | date                            | lang             | last | name
--------+---------------------------------+------------------+------+------
      1 | 2012-03-11 18:30:00.000000+0000 | ['tamil', 'eng'] | 89.9 | Ashu

(1 rows)
cqlsh:students> begin batch insert into student(rollno,name,date, last) values(2,'Ash','2012-03-12',89.9)using ttl 30; apply batch;
cqlsh:students> select * from student;

 rollno | date                            | lang             | last | name
--------+---------------------------------+------------------+------+------
      1 | 2012-03-11 18:30:00.000000+0000 | ['tamil', 'eng'] | 89.9 | Ashu
      2 | 2012-03-11 18:30:00.000000+0000 |             null | 89.9 |  Ash

(2 rows)
cqlsh:students> select ttl(rollno) from student where rollno=1
            ... 
cqlsh:students> select ttl(rollno) from student where rollno=1;
InvalidRequest: Error from server: code=2200 [Invalid query] message="Cannot use selection function ttl on PRIMARY KEY part rollno"
cqlsh:students> select ttl(name) from student where rollno=1;

 ttl(name)
-----------

(0 rows)
cqlsh:students> select * from student;

 rollno | date | lang             | last | name
--------+------+------------------+------+------
      1 | null | ['tamil', 'eng'] | null | null

(1 rows)
cqlsh:students> begin batch insert into student(rollno,name,date, last) values(2,'Ash','2012-03-12',89.9)using ttl 30; apply batch;
cqlsh:students> select * from student;

 rollno | date                            | lang             | last | name
--------+---------------------------------+------------------+------+------
      1 |                            null | ['tamil', 'eng'] | null | null
      2 | 2012-03-11 18:30:00.000000+0000 |             null | 89.9 |  Ash

(2 rows)
cqlsh:students> select ttl(name) from student where rollno=2;

 ttl(name)
-----------
        21

(1 rows)
cqlsh:students> copy student(rollno,name,date,last) to 'd:/in.csv';
Using 16 child processes
Can't open 'd:/in.csv' for writing: [Errno 2] No such file or directory: 'd:/in.csv'
cqlsh:students> update student set rollno=2 where name='dhanush';
InvalidRequest: Error from server: code=2200 [Invalid query] message="PRIMARY KEY part rollno found in SET part"
cqlsh:students> update student set last=2 where name='dhanush';
InvalidRequest: Error from server: code=2200 [Invalid query] message="Some partition key parts are missing: rollno"
cqlsh:students> create index on student(name);
cqlsh:students> update student set last=2 where name='dhanush';
InvalidRequest: Error from server: code=2200 [Invalid query] message="Some partition key parts are missing: rollno"
cqlsh:students> update student set last=2 where rollno=2 and name='dhanush';
InvalidRequest: Error from server: code=2200 [Invalid query] message="Non PRIMARY KEY columns found in where clause: name "
cqlsh:students> 
cqlsh:students> 

